{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terraform script to initiate S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider \"aws\" {\n",
    "  region = \"us-east-2\"  # Replace with your desired AWS region\n",
    "}\n",
    " \n",
    "resource \"aws_s3_bucket\" \"my_bucket\" {\n",
    "  bucket = \"scene-test2\"  # Replace with a globally unique bucket name\n",
    "  acl    = \"private\"  # You can set the ACL (Access Control List) as needed\n",
    " \n",
    "  tags = {\n",
    "    Name = \"My S3 Bucket\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glue Job script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider \"aws\" {\n",
    "  region = \"us-east-2\"  # Change to your desired AWS region\n",
    "}\n",
    "\n",
    "# IAM Role for Glue Job (Create as described in the previous answer)\n",
    "\n",
    "# Glue Job\n",
    "resource \"aws_glue_job\" \"glue_read_csv_job\" {\n",
    "  name        = \"glue-read-csv-job\"\n",
    "  role_arn    = aws_iam_role.glue_role.arn\n",
    "  command {\n",
    "    name        = \"glueetl\"\n",
    "    python_script = filebase64(\"nb2.py\")  # Path to the Glue job script\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data checking through Glue PySpark Notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "  \n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "source_path = \"s3://scene-test1-s3/whitewines.csv\"  # Replace with your S3 bucket and CSV file path\n",
    "dynamic_frame = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"paths\": [source_path]},\n",
    "    format=\"csv\",\n",
    "    format_options={\"withHeader\": \"true\"}\n",
    ")\n",
    "dynamic_frame.show()\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
